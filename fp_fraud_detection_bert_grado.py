import os
import pathlib
from collections import namedtuple
from collections.abc import Callable

import gradio
import pandas as pd
import sklearn
import sklearn.metrics
import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EvalPrediction


CWD = pathlib.Path(__file__).parent.resolve()
_MODEL_DIR = CWD.joinpath(os.environ.get('MODEL_PATH_DIR_NAME', default="fraud_bert_model"))
_MODEL_BIN_NAME = os.environ.get('MODEL_BIN_NAME', default='pytorch_model.bin')
_MODEL_SAFE_NAME = os.environ.get('MODEL_BIN_NAME', default='model.safetensors')


PreDataSet = namedtuple('DataSetTuple', field_names=(
    'train_texts',
    'val_texts',
    'train_labels',
    'val_labels',
))


class FinancialFraudDataset(torch.utils.data.Dataset):
    """
    è‡ªå®šç¾© Dataset é¡åˆ¥ï¼Œç”¨æ–¼å°‡æ–‡æœ¬å’Œæ¨™ç±¤è½‰æ›ç‚º PyTorch èƒ½è™•ç†çš„æ ¼å¼ã€‚
    """
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        # å°‡æ¯ç­†è³‡æ–™è½‰æ›ç‚º tensorï¼ŒåŒ…å« token ç·¨ç¢¼åŠå°æ‡‰çš„æ¨™ç±¤
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item


def is_model_trained(
    model_dir_name: str = 'fraud_bert_model',
    model_bin_name: str = 'pytorch_model.bin',
    model_safe_name: str = 'model.safetensors',
) -> bool:
    model_dir = CWD.joinpath(model_dir_name)

    if not model_dir.exists():
        model_dir.mkdir()

    if any((
        model_dir.joinpath(model_bin_name).exists(),
        model_dir.joinpath(model_safe_name).exists(),
    )):
        print("âœ… Model already trained.")
        return True

    print("âŒ Model not trained yet.")
    return False


def prepare_dataset(data_path: str = None):
    path = CWD.joinpath(data_path or 'fraud_detection_sample.csv')

    if not path.is_file():
        raise Exception(f"{path} file not exists.")

    df = pd.read_csv(str(path), encoding="utf-8")

    dataset_list = sklearn.model_selection.train_test_split(
        df['text'].tolist(),
        df['label'].tolist(),
        test_size=0.2,
        random_state=42,
    )

    return PreDataSet(
        train_texts=dataset_list[0],
        val_texts=dataset_list[0],
        train_labels=dataset_list[0],
        val_labels=dataset_list[0],
    )


def train_model(
    dataset: PreDataSet,
    tokenizer: BertTokenizer = None,
    model: BertForSequenceClassification = None,
    train_args: TrainingArguments = None,
    compute_metrics: Callable[[EvalPrediction], dict] = None,
    model_save_dir: str | pathlib.Path = 'fraud_bert_model',
) -> pathlib.Path:
    """Train model with dataset then return trained model data path"""
    def _compute_metrics(pred: EvalPrediction):
        # è¨ˆç®— accuracyã€precisionã€recallã€F1 åˆ†æ•¸
        labels = pred.label_ids
        preds = pred.predictions.argmax(-1)
        acc = sklearn.metrics.accuracy_score(labels, preds)
        precision, recall, f1, _ = sklearn.metrics.precision_recall_fscore_support(
            y_true=labels,
            y_pred=preds,
            average="binary",
        )
        return dict(
            accuracy=acc,
            precision=precision,
            recall=recall,
            f1=f1,
        )

    tokenizer = tokenizer or BertTokenizer.from_pretrained("hfl/chinese-roberta-wwm-ext")
    model = model or BertForSequenceClassification.from_pretrained("hfl/chinese-roberta-wwm-ext", num_labels=2)
    train_args = train_args or TrainingArguments(
        output_dir="./results",        # è¨“ç·´çµæœå„²å­˜ä½ç½®
        num_train_epochs=20,           # è¨“ç·´è¼ªæ•¸
        per_device_train_batch_size=4, # æ¯æ‰¹è¨“ç·´æ•¸é‡
        per_device_eval_batch_size=4,  # æ¯æ‰¹é©—è­‰æ•¸é‡
        warmup_steps=10,               # é ç†±æ­¥é©Ÿæ•¸
        weight_decay=0.01,             # æ¬Šé‡è¡°é€€
        logging_dir="./logs",          # æ—¥èªŒå„²å­˜ä½ç½®
        logging_steps=10,              # æ—¥èªŒç´€éŒ„é »ç‡
        report_to="none"               # ä¸ä½¿ç”¨å¤–éƒ¨å·¥å…·å ±å‘Šè¨“ç·´éç¨‹
    )
    compute_metrics = compute_metrics or _compute_metrics

    train_dataset = FinancialFraudDataset(
        encodings=tokenizer(dataset.train_texts, truncation=True, padding=True, max_length=128),
        labels=dataset.train_labels,
    )
    val_dataset = FinancialFraudDataset(
        encodings=tokenizer(dataset.val_texts, truncation=True, padding=True, max_length=128),
        labels=dataset.val_labels,
    )

    trainer = Trainer(
        model=model,
        args=train_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics, # è¨ˆç®—è©•ä¼°æŒ‡æ¨™
    )
    trainer.train()

    # Save model and tokenizer
    model_dir = CWD.joinpath(model_save_dir) if isinstance(model_save_dir, str) else model_save_dir
    model.save_pretrained(str(model_dir))
    tokenizer.save_pretrained(str(model_dir))

    return model_dir


def show_training_result_with_gradio(model_dir: pathlib.Path):
    model = BertForSequenceClassification.from_pretrained(str(model_dir))
    model.eval()
    tokenizer = BertTokenizer.from_pretrained(str(model_dir))

    def _predict_transaction(inp: str):
        try:
            with torch.no_grad():
                inputs = tokenizer(inp, return_tensors="pt", truncation=True, padding=True, max_length=128)
                outputs = model(**inputs)
                probs = torch.softmax(outputs.logits, dim=1)  # æ©Ÿç‡åˆ†å¸ƒ
                prediction = torch.argmax(probs, dim=1).item()

            confidence = probs[0][prediction].item()
            label = "âœ… Legitimate" if prediction == 0 else "âš ï¸ Fraudulent"
            return f"{label}  (Confidence: {confidence:.2f})"

        except Exception as e:
            return f"Error: {str(e)}"
    
    gradio.Interface(
        fn=_predict_transaction,   # æŒ‡å®šæ¨è«–å‡½å¼
        inputs=gradio.Textbox(lines=3, placeholder="è¼¸å…¥äº¤æ˜“ç°¡è¨Š..."),
        outputs="text",
        title="ğŸ’³ ä¸­è‹±æ–‡è©é¨™ç°¡è¨Šåˆ¤æ–·å™¨",
        description="è¼¸å…¥äº¤æ˜“ç›¸é—œè¨Šæ¯ï¼Œåˆ¤æ–·æ˜¯å¦ç‚ºè©é¨™è¨Šæ¯ï¼ˆæ”¯æ´ä¸­æ–‡èˆ‡è‹±æ–‡ï¼‰ã€‚"
    ).launch(share=False) # å¦‚æœé˜²æ¯’è»Ÿé«”æœƒå ±éŒ¯ï¼Œè«‹å°‡share=True, debug=Trueæ”¹ç‚ºshare=False


def main():
    model_dir_name = os.environ.get('MODEL_DIR_NAME')
    model_bin_name = os.environ.get('MODEL_BIN_NAME')
    model_safe_name = os.environ.get('MODEL_SAFE_NAME')
    dataset_path = os.environ.get('DATA_PATH')

    model_dir = CWD.joinpath(model_dir_name)

    if not is_model_trained(
        model_dir_name,
        model_bin_name,
        model_safe_name,
    ):
        # TRAIN data
        dataset = prepare_dataset(dataset_path)
        model_dir = train_model(dataset)
    
    show_training_result_with_gradio(model_dir)
