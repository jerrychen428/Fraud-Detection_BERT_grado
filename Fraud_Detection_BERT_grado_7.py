import os
import torch
import pandas as pd
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import gradio as gr

class FinancialFraudDataset(Dataset):
    """
    è‡ªå®šç¾© Dataset é¡åˆ¥ï¼Œç”¨æ–¼å°‡æ–‡æœ¬å’Œæ¨™ç±¤è½‰æ›ç‚º PyTorch èƒ½è™•ç†çš„æ ¼å¼ã€‚
    """
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        # å°‡æ¯ç­†è³‡æ–™è½‰æ›ç‚º tensorï¼ŒåŒ…å« token ç·¨ç¢¼åŠå°æ‡‰çš„æ¨™ç±¤
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

class FinancialFraudTrainer:
    def __init__(self):
        self.data_path = None
        self.train_texts = None
        self.val_texts = None
        self.train_labels = None
        self.val_labels = None
        self.tokenizer = None
        self.train_dataset = None
        self.val_dataset = None
        self.model = None

    def prepare_dataset(self, file_path="./fraud_detection_sample.csv"):
        # è®€å– CSV æª”æ¡ˆï¼Œä½¿ç”¨ UTF-8 ç·¨ç¢¼
        self.data_path = file_path or self.data_path
        df = pd.read_csv(self.data_path, encoding="utf-8")
        # åˆ†å‰²ç‚ºè¨“ç·´é›†èˆ‡é©—è­‰é›†
        self.train_texts, self.val_texts, self.train_labels, self.val_labels = train_test_split(
            df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42)

    def tokenize_data(self, pretrained="hfl/chinese-roberta-wwm-ext"):
        # è¼‰å…¥ä¸­æ–‡ RoBERTa tokenizer
        self.tokenizer = BertTokenizer.from_pretrained(pretrained)
        # å°è¨“ç·´èˆ‡é©—è­‰æ–‡æœ¬é€²è¡Œç·¨ç¢¼
        train_encodings = self.tokenizer(self.train_texts, truncation=True, padding=True, max_length=128)
        val_encodings = self.tokenizer(self.val_texts, truncation=True, padding=True, max_length=128)
        # å°è£æˆ Dataset
        self.train_dataset = FinancialFraudDataset(train_encodings, self.train_labels)
        self.val_dataset = FinancialFraudDataset(val_encodings, self.val_labels)

    def load_model(self, pretrained="hfl/chinese-roberta-wwm-ext"):
        # è¼‰å…¥ä¸­æ–‡ RoBERTa åˆ†é¡æ¨¡å‹ï¼Œè¨­å®šåˆ†é¡æ•¸ç‚º 2ï¼ˆåˆæ³• / è©é¨™ï¼‰
        self.model = BertForSequenceClassification.from_pretrained(pretrained, num_labels=2)

    def train_model(self, output_dir="./results"):
        # è¨­å®šè¨“ç·´åƒæ•¸
        training_args = TrainingArguments(
            output_dir=output_dir,                # è¨“ç·´çµæœå„²å­˜ä½ç½®
            num_train_epochs=20,                    # è¨“ç·´è¼ªæ•¸
            per_device_train_batch_size=4,          # æ¯æ‰¹è¨“ç·´æ•¸é‡
            per_device_eval_batch_size=4,           # æ¯æ‰¹é©—è­‰æ•¸é‡
            warmup_steps=10,                         # é ç†±æ­¥é©Ÿæ•¸
            weight_decay=0.01,                       # æ¬Šé‡è¡°é€€
            logging_dir="./logs",                  # æ—¥èªŒå„²å­˜ä½ç½®
            logging_steps=10,                        # æ—¥èªŒç´€éŒ„é »ç‡
            report_to="none"                         # ä¸ä½¿ç”¨å¤–éƒ¨å·¥å…·å ±å‘Šè¨“ç·´éç¨‹
        )

        # å®šç¾© Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.val_dataset,
            compute_metrics=self.compute_metrics   # è¨ˆç®—è©•ä¼°æŒ‡æ¨™
        )

        # åŸ·è¡Œè¨“ç·´
        trainer.train()

    def compute_metrics(self, pred):
        # è¨ˆç®— accuracyã€precisionã€recallã€F1 åˆ†æ•¸
        labels = pred.label_ids
        preds = pred.predictions.argmax(-1)
        acc = accuracy_score(labels, preds)
        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="binary")
        return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

    def save_model(self):
        # å„²å­˜æ¨¡å‹èˆ‡ tokenizer
        self.model.save_pretrained("fraud_bert_model")
        self.tokenizer.save_pretrained("fraud_bert_model")

    def load_saved_model(self):
        # é‡æ–°è¼‰å…¥å·²å„²å­˜çš„æ¨¡å‹èˆ‡ tokenizerï¼Œä¾›æ¨è«–ä½¿ç”¨
        self.model = BertForSequenceClassification.from_pretrained("fraud_bert_model")
        self.tokenizer = BertTokenizer.from_pretrained("fraud_bert_model")
        self.model.eval()

    def predict_transaction(self, text):
        # å–®ç­†æ¨è«–ç”¨ï¼Œå›å‚³é æ¸¬çµæœèˆ‡ä¿¡å¿ƒåˆ†æ•¸
        try:
            self.model.eval()
            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
            with torch.no_grad():
                outputs = self.model(**inputs)
                probs = torch.softmax(outputs.logits, dim=1)  # æ©Ÿç‡åˆ†å¸ƒ
                prediction = torch.argmax(probs, dim=1).item()
                confidence = probs[0][prediction].item()
            label = "âœ… Legitimate" if prediction == 0 else "âš ï¸ Fraudulent"
            return f"{label}  (Confidence: {confidence:.2f})"
        except Exception as e:
            return f"Error: {str(e)}"

    def launch_gradio(self):
        # ä½¿ç”¨ Gradio éƒ¨ç½²ç¶²é ä»‹é¢
        gr.Interface(
            fn=self.predict_transaction,   # æŒ‡å®šæ¨è«–å‡½å¼
            inputs=gr.Textbox(lines=3, placeholder="è¼¸å…¥äº¤æ˜“ç°¡è¨Š..."),
            outputs="text",
            title="ğŸ’³ ä¸­è‹±æ–‡è©é¨™ç°¡è¨Šåˆ¤æ–·å™¨",
            description="è¼¸å…¥äº¤æ˜“ç›¸é—œè¨Šæ¯ï¼Œåˆ¤æ–·æ˜¯å¦ç‚ºè©é¨™è¨Šæ¯ï¼ˆæ”¯æ´ä¸­æ–‡èˆ‡è‹±æ–‡ï¼‰ã€‚"
        ).launch(share=False) # å¦‚æœé˜²æ¯’è»Ÿé«”æœƒå ±éŒ¯ï¼Œè«‹å°‡share=True, debug=Trueæ”¹ç‚ºshare=False

    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¶“è¨“ç·´éfunction
    def model_already_trained(self):
        print("Checking if model already trained...")
        model_path_bin = os.path.join("fraud_bert_model", "pytorch_model.bin")
        model_path_safe = os.path.join("fraud_bert_model", "model.safetensors")

        if any([
            os.path.exists(model_path_bin),
            os.path.exists(model_path_safe),
        ]):
            print("âœ… Model already trained.")
            return True

        print("âŒ Model not trained yet.")
        return False

if __name__ == "__main__":
    # å»ºç«‹ Trainer å¯¦ä¾‹
    trainer = FinancialFraudTrainer()
    if not trainer.model_already_trained():# æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²è¨“ç·´
        trainer.prepare_dataset()       # è³‡æ–™å‰è™•ç†
        trainer.tokenize_data()         # æ–‡å­—ç·¨ç¢¼
        trainer.load_model()            # è¼‰å…¥æ¨¡å‹
        trainer.train_model()           # æ¨¡å‹è¨“ç·´
        trainer.save_model()            # å„²å­˜æ¨¡å‹

    trainer.load_saved_model()  # è¼‰å…¥æ¨¡å‹ä¾›é æ¸¬
    trainer.launch_gradio()     # å•Ÿå‹• Gradio ç¶²é ä»‹é¢
